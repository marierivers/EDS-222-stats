<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Summarizing data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Tamma Carleton" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Summarizing data
## EDS 222
### Tamma Carleton
### Fall 2021

---




name: Overview

# Today

####  Types of variables
- Categorical, numerical, ordinal, ... 

--

#### Probability density functions
- Definitions, the normal pdf, skew

--

#### Summary statistics
- Central tendency and spread, quantiles, outliers

--

#### Law of large numbers
- How big does my sample need to be? 

--

#### Relationships between variables
- Covariance, correlation

---

layout: false
class: clear, middle, inverse
# Assignment #1 check-in: How's it going?

### Reminder: OH today, 3418 Bren Hall, 3-4pm

---
layout: false
class: clear, middle, inverse
# Types of variables
---

# Types of variables

## Numerical variables
&gt; Object class `numeric` in `R`
- Can take on a wide range of possible values
- Makes sense to add, subtract, multiply, etc.

--
 
- Examples: 
  + Height of the tree canopy across the Amazon
  + Length of Atlantic swordfish
  + Daily average temperature

#### **Discrete** numerical variables take on only a limited set of values, often counts (e.g., population)
#### **Continuous** numerical variables: can take on infinite values within a range (e.g., arsenic concentration in groundwater)


---

# Types of variables

## Numerical variables

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="numerical.jpg" alt="Source: Allison Horst" width="70%" /&gt;
&lt;p class="caption"&gt;Source: Allison Horst&lt;/p&gt;
&lt;/div&gt;

---

# Types of variables

## Categorical variables
&gt; Object class `factor` in `R`

- Values correspond to one of a fixed number of categories
- Possible values are called **levels** 

--
 
- Examples: 
  + States of the US
  + Species of tree
  + Age group (e.g., &lt;15, 15-64, 65+) (watch out! continuous numerical data can often be stored as a categorical variable!)

---
# Types of variables

## Categorical variables

#### **Nominal** variables are unordered descriptions
#### **Ordinal** variables are categories with a natural ordering
#### **Binary** variables only take on 0 or 1 

---

# Types of variables

## Categorical variables

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="categorical.jpg" alt="Source: Allison Horst" width="85%" /&gt;
&lt;p class="caption"&gt;Source: Allison Horst&lt;/p&gt;
&lt;/div&gt;

---
layout: false
class: clear, middle, inverse
# Probability density functions

---
# Probability density functions

For _continuous_ variables, the **probability density function (p.d.f.)** tells us the probability that a random variable falls within a given range of values. 

Formally: The **p.d.f.** of a continuous variable `\(X\)` with support (i.e., range of possible values) `\(S\)` is an integrable function `\(f(x)\)` satisfying:

--

1. `\(f(x)\)` is positive for all `\(x\)` in `\(S\)`

--

2. The area under the curve `\(f(x)\)` over the entire support `\(S\)` is equal to 1: $$ \int_S f(x)dx = 1$$

--

3. The probability that `\(x\)` falls between `\(A\)` and `\(B\)` is: $$ Pr(A\leq x \leq B) = \int_A^B f(x)dx $$

---
# Why isn't this simpler?

&gt; Q: Why can't I just interpret `\(f(x)\)` as the probability that `\(X=x\)`?

&gt; A: Because continuous variables have `\(\infty\)` possible values...the probability that your variable `\(X\)` exactly equals `\(x\)` is zero! 

--

### Luckily, for **discrete variables** it _is_ this simple!

For _discrete_ variable `\(x\)` , the **probability mass function (p.m.f.)** `\(f(x)\)` tells us the probability that `\(X = x\)`.

Formally: The **p.m.f.** of a discrete variable `\(X\)` with support (i.e., range of possible values) `\(S\)` is a function `\(f(x)\)` satisfying:

1. `\(P(X=x) = f(x) &gt;0\)` for all `\(x\)` in support `\(S\)`

2. `\(\sum_{x\in S} f(x) = 1\)`

3. `\(P(A\leq x \leq B) = \sum_{x=A}^{x=B} f(x)\)`

---
# Probability density functions (visual)

P.d.f.'s help us characterize the distribution of our population. The most common/famous ones get names (e.g., normal, Gamma, `\(t\)`,...)

### Let's look at a **normal** distribution*
The probability this normally distributed variable takes on a value between -2 and 0 is shown in pink:

&lt;img src="02-summstats_files/figure-html/examplepdf-1.svg" style="display: block; margin: auto;" /&gt;

`\(^*\)`This distribution happens to be what's called "standard" normal. We'll get into the weeds later!
---
# Probability density functions (visual)

### Let's look at a **normal** distribution*
The probability this normally distributed variable takes on a value between -2 and 2 is shown in pink:

&lt;img src="02-summstats_files/figure-html/examplepdf2-1.svg" style="display: block; margin: auto;" /&gt;

`\(^*\)`Yep, still a "standard" normal. Details later. 
---
# The normal distribution

There are infinite different normal distributions. They all have the following p.d.f.:

$$f(x) =\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)} $$
where `\(\mu\)` is the mean (i.e., average) and `\(\sigma\)` is the standard deviation (will define soon).

&lt;img src="normals.png" width="50%" style="display: block; margin: auto;" /&gt;
#### Many results in statistics rely on the assumption that our data are normally distributed. We will return to this distribution **frequently!**

---
# Shapes of probability distributions

Key terms to describe p.d.f.'s:

1. A distribution can have **skew** (e.g., log-normal)
2. A distribution can have a long **right tail** or **left tail** (e.g., fat-tailed climate sensitivity distributions!)
3. A distribution can be **symmetric**
4. A distribution can be **unimodal**, **bimodal**, or **multimodal**

---
# Shapes of probability distributions

## Skew with a long right tail 
#### (log-normal sample distribution)
&lt;img src="02-summstats_files/figure-html/exampleskew-1.svg" style="display: block; margin: auto;" /&gt;

---
# Shapes of probability distributions

## Uni-, bi-, and multi-modal
#### (How many "peaks" do you see?)
&lt;img src="02-summstats_files/figure-html/examplebimodal-1.svg" style="display: block; margin: auto;" /&gt;
---
layout: false
class: clear, middle, inverse
# Summary statistics
---

# Describing random variables

A probability density function describes a **population**

As we learned last week, we rarely have a **census** so we rarely can directly describe the p.d.f. itself.

#### Instead, we use _statistics_ from a _sample_ to estimate _parameters_ of the _population_

&lt;img src="02-summstats_files/figure-html/examplepdf3-1.svg" style="display: block; margin: auto;" /&gt;

---
# Measures of central tendency

### We often begin to describe a distribution using measures of **central tendency** (i.e., measures of the "middle").

Three are most common:
1. **Mean** 
2. **Median**
3. **Mode**

---
# Mean = expected value = average

In a **population**, the mean is defined as: `$$\mathrm{E}[X]=\mu=\int_S xf(x)dx$$`

--

In our **sample**, we compute the mean as: `$$\bar{x}=\frac{1}{n}\sum_{i\in n} x_i$$`

#### We use `\(\bar{x}\)` as an *estimate* of the parameter of interest, `\(\mu\)`.

&lt;img src="02-summstats_files/figure-html/examplemean-1.svg" style="display: block; margin: auto;" /&gt;
---
# Median = middle value 

In a **population**, the median `\(m\)` is defined as: `$$P(X\leq m) = \int_{-\infty}^m f(x)dx = \frac{1}{2} = \int_m^{\infty} f(x)dx = P(X\geq m)$$`

--

In our **sample**, we compute the median as: 
- `\(n\)` even? median = mean of the middle two values
- `\(n\)` odd? median = middle value

&lt;img src="02-summstats_files/figure-html/examplemedian-1.svg" style="display: block; margin: auto;" /&gt;
---
# Median and mean are not always close

#### Non-normal distribution `\(\implies\)` median and mean can diverge substantially

&lt;img src="02-summstats_files/figure-html/examplemedian2-1.svg" style="display: block; margin: auto;" /&gt;

---
# Mode = most frequent value

### The **mode** is simply the most frequently observed value 

This is much more useful for discrete data (ask yourself why!)

&lt;img src="02-summstats_files/figure-html/examplemode-1.svg" style="display: block; margin: auto;" /&gt;

---
# Measures of spread

### Central tendency only gets us so far...we also need measures of **spread**.

1. **Range** (easy: min to max of your data)
2. **Variance**
3. **Standard deviation**
4. **Quantiles**

---

# Measures of spread: Variance

Answers the question, how far are observations from the mean, on average? 

In the population: `$$Var(X) = \mathrm{E}[(X-\mu)^2] = \sigma^2 = \int_{\mathrm S} (x-\mu)^2f(x)dx$$`

In the sample: $$ s^2 = \frac{\sum_{i \in n}(x_i-\bar{x})^2}{n-1}$$
&gt; Q: Why do we divide by `\(n-1\)`? 

&gt; A: Lots of math to prove it (see [here](https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/more-standard-deviation/v/review-and-intuition-why-we-divide-by-n-1-for-the-unbiased-sample-variance)), but trust me, `\(s^2\)` will be a biased estimate of `\(\sigma^2\)` if you divide by `\(n\)`!

#### Units of variance: units of the random variable, _squared_

---
# Measures of spread: Standard deviation 

Just the square root of the variance!  

In the population: `$$SD(X) = \sqrt{\mathrm{E}[(X-\mu)^2]} = \sigma = \sqrt{\int_{\mathrm S} (x-\mu)^2f(x)dx}$$`

In the sample: $$ s = \sqrt{\frac{1}{n-1}\sum_{i \in n}(x_i-\bar{x})^2}$$

#### Units of standard deviation: units of the random variable

---

# Some helpful rules

$$\mathrm{E}[aX+b] = a\mathrm{E}[X] + b $$
$$\mathrm{E}[X+Y] = \mathrm{E}[X] + \mathrm{E}[Y] $$
`$$var(X) = \mathrm{E}[X^2] - (\mathrm{E}[X])^2$$`
`$$var(aX+b) = a^2var(X)$$`

---
# Variance, visually
 

**Pink**: Low variance/standard deviation `\(\sigma = 1\)`

**Green**: High variance/standard deviation `\(\sigma = 2\)`

&lt;img src="02-summstats_files/figure-html/low variance-1.svg" style="display: block; margin: auto;" /&gt;

---
# Variance, visually

#### Back to the normal distributions
- Changes in the _mean_ shift the distribution right to left
- Changes in the _standard deviation_ stretch the distribution out (or shrink it in)

&lt;img src="normals.png" width="70%" style="display: block; margin: auto;" /&gt;
---
# Measures of spread: Quantiles

### Quantiles are cut points of a probability distribution

In our sample, quantiles are cut points of our sample data

#### How do we compute them?

- We order our data from lowest to highest

- For the `\(q\)`-quantile, we divide these ordered data into `\(q\)` equal sized subsamples 
- The value at the edge of the `\(k\)`th subsample is the `\(k\)`th `\(q\)`-quantile
  + This tells you the value below which `\(\frac{k}{q}\)` of the data lie

--

&gt; Question: How many `\(q\)`-quantiles are there for any given `\(q\)`?

--

&gt; Answer: There are `\(q-1\)` of the `\(q\)`-quantiles

---
# Example: The normal distribution

Common quantiles have names you have head of, such as _quartiles_ for `\(q=4\)`:

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="quantiles_normal.png" alt="Quartiles of the normal distribution" width="50%" /&gt;
&lt;p class="caption"&gt;Quartiles of the normal distribution&lt;/p&gt;
&lt;/div&gt;

**Interpretation:** Q1 = first quartile, Q2 = second quartile, etc. The area below the red curve is the same below Q1 as it is between Q1 and Q2, between Q2 and Q3, and above Q3. 
---
# Common quantiles and interpretation

### Common quantiles have names you have heard of: 
- `\(q=2\)` **Median** tells us the value for which 50% of our sample sits _below_ (and 50% above). This is quantile 0.5 (or 50% quantile)
- `\(q=3\)` **Terciles**: tell us the values for which 33.33% (1st tercile) and 66.66% (2nd tercile) of our sample sits _below_
- `\(q=4\)` **Quartiles**: tell us the values for which 25% (1st quartile), 50% (2nd quartile), and 75% (3rd quartile) of our sample sits _below_
- `\(q=10\)` **Deciles**: tell us the values for which 10% (1st decile), ..., 50% (5th decile), ..., and 90% (9th decile) of our sample sits _below_

--

`\(q\)` The k_th_ `\(q\)`-quantile tells us the value for which `\(\frac{k}{q}\times 100\)`% of our sample sits _below_


---
# This sounds a lot like percentiles...

### Percentiles are simply quantiles for q=100!

#### We hear about percentiles in daily life more often, and in practice people often use "percentiles" language for the more general term "quantiles". 

#### Examples of percentiles:
- At 5'3", my height is the 40th percentile of the U.S. adult female height distribution `\(\rightarrow\)` 40% of American female adults are shorter than me
- At 24.5 lbs, my son is the 81st percentile of U.S. male 13 month old weight distribution `\(\rightarrow\)` 81% of American male 13 month olds are lighter than my son

&gt; Exercise: Draw approximately where you think the 1st, 10th, 20th, 50th, 80th, 90th and 99th percentiles would be on a normal distribution. 

---
# Quantile-Quantile (Q-Q) Plots

### **Histograms** plot the frequency of our data within bins
  - `geom_histogram()` with `ggplot2` in `R`
  
### **Q-Q plots** plot the quantiles of our data _against_ quantiles of some theoretical distribution
  - `geom_qq()` with `ggplot2` in `R`

&gt; This is helpful if we want to ask things like, are my data approximately normally distributed? 

#### Straight line on a Q-Q plot indicates sample and theoretical distributions match

---
# Q-Q plot: Example

### Annual flow of the river Nile at Aswan, 1871-1970, in 10^8 m^3

.pull-left[
&lt;img src="02-summstats_files/figure-html/nilehist-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="02-summstats_files/figure-html/nileqq-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Q-Q plot: Example

### Monthly mean relative sunspot numbers, 1749-1983

.pull-left[
&lt;img src="02-summstats_files/figure-html/sunspots-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="02-summstats_files/figure-html/sunspots2-1.svg" style="display: block; margin: auto;" /&gt;
]


&gt; We will continually return to the normal distribution. Always a good idea to check whether your data look normally distributed or not!

---
# Which statistics are robust to outliers?

- Consider a sample of loans from a bank, each with an associated interest rate. 
  + `\(\bar x = 11.57%\)`
  + `\(s=5.05%\)`
  
- The highest value in the data is somewhat of an outlier, `\(x_{max} = 26.3%\)`.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="loan_distribution.png" alt="Source: IMS, Ch. 5.6" width="80%" /&gt;
&lt;p class="caption"&gt;Source: IMS, Ch. 5.6&lt;/p&gt;
&lt;/div&gt;

---
# Which statistics are robust to outliers?

- Consider a sample of loans from a bank, each with an associated interest rate. 
  + `\(\bar x = 11.57%\)`
  + `\(s=5.05%\)`
  
- The highest value in the data is somewhat of an outlier, `\(x_{max} = 26.3%\)`.

- How do summary statistics change if we modify this outlier? 

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="robusttable.png" alt="Source: IMS, Ch. 5.6" width="80%" /&gt;
&lt;p class="caption"&gt;Source: IMS, Ch. 5.6&lt;/p&gt;
&lt;/div&gt;

---
layout: false
class: clear, middle, inverse
# Law of large numbers
---
# Big data

#### You probably have intuition that a larger sample is better than a smaller one...but why? 

Suppose we have a **random** sample of some size `\(n\)`. How well does `\(\bar x\)` approximate `\(\mu\)`?  

### Law of large numbers:

$$\bar{x} \rightarrow \mu \hskip2mm \text{as} \hskip2mm n \rightarrow \infty $$
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="lln.png" alt="Source: IMS, Ch. 5.6" width="45%" /&gt;
&lt;p class="caption"&gt;Source: IMS, Ch. 5.6&lt;/p&gt;
&lt;/div&gt;
---
layout: false
class: clear, middle, inverse
# Relationships between variables
---
# Two random variables

### Often we are interested in the _relationship_ between two (or more) random variables.
E.g., heat waves and heart attacks, nitrogen fertilizer and water pollution

--

&lt;img src="02-summstats_files/figure-html/scatter1-1.svg" style="display: block; margin: auto;" /&gt;

*Note: these are simulated data. But the violence-temperature link is real! See [here](https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-080614-115430) for a summary of research.
---
# Two random variables

### What metrics can we use to characterize the _relationship_ between two variables? 

There are lots. But let's start with...

--

#### 1. Covariance


#### 2. Correlation

&lt;img src="02-summstats_files/figure-html/scatter2-1.svg" style="display: block; margin: auto;" /&gt;
---
# Covariance 

#### **Variance** indicates how dispersed a distribution is (average squared deviation from the mean)

--

#### **Covariance** is a measure of the _joint_ distribution of two variables 

- Higher values of `\(X\)` correspond to higher values of `\(Y\)` `\(\rightarrow\)` **positive** covariance
- Higher values of `\(X\)` correspond to lower values of `\(Y\)` `\(\rightarrow\)` **negative** covariance 

--

In the population:
`$$Cov(X,Y) = E[(X-\mu_x)(Y-\mu_y)] = E[XY]-\mu_x\mu_y$$`

In the sample:
`$$s_{xy} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)$$`

#### The **sign** of `\(s_{xy}\)` tells us the sign of the linear relationship between `\(X\)` and `\(Y\)`, but the **magnitude** depends on the units of the variables and is therefore difficult to interpret

---
# Covariance

### Example: positive covariance
&lt;img src="02-summstats_files/figure-html/covar1-1.svg" style="display: block; margin: auto;" /&gt;

---
# Covariance

### Example: zero covariance

&lt;img src="02-summstats_files/figure-html/covar2-1.svg" style="display: block; margin: auto;" /&gt;


---
# Covariance

### Example: Negative covariance

&lt;img src="02-summstats_files/figure-html/covar3-1.svg" style="display: block; margin: auto;" /&gt;

How do I interpret these units?! Hard to compare across these three graphs...

---

# Correlation

#### **Correlation** allows us to normalize covariance into interpretable units
The sign still tells us about the nature of the (linear) relationship between two variables:
  
  - **positive** covariance `\(\rightarrow\)` **positive** correlation (and vice versa)

But now, the magnitude is interpretable:
  
  - Ranges from -1 to 1, with magnitude indicating _strength_ of the relationship

---
# Correlation

#### **Correlation** allows us to normalize covariance into interpretable units  
  
  In the population:
  `$$\rho_{X,Y} = corr(X,Y) = \frac{cov(X,Y)}{\sigma_x \sigma_y}$$`
  
  In the sample:
  `$$r_{x,y} = \frac{s_{x,y}}{s_x s_y} = \frac{1}{(n-1)s_x s_y}\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)$$`
  
---
  
# Correlation
  
### Example: Strong positive correlation

&lt;img src="02-summstats_files/figure-html/corr1-1.svg" style="display: block; margin: auto;" /&gt;


---
# Correlation
  
### Example: zero correlation
  
&lt;img src="02-summstats_files/figure-html/corr2-1.svg" style="display: block; margin: auto;" /&gt;

---
# Correlation
  
### Example: Moderate negative correlation
  
&lt;img src="02-summstats_files/figure-html/corr3-1.svg" style="display: block; margin: auto;" /&gt;

---
# Next up

### Summary statistics in `R` (Thursday lab, Assignment 02)

### Linear regression (Next Tuesday)

---

class: center, middle


Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Some slide components were borrowed from [Ed Rubin's](https://github.com/edrubin/EC421S20) awesome course materials.

---
exclude: true


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
